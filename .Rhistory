getwd()
install.packages("ISLR")
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
Boston
data = boston
boston = read.csv("boston.csv")
fitpoly = glm(NOX ~ poly(DIS), data = boston)
fitpoly$coefficients
summary(fitpoly)
par(mfrow = c(1,2))
plot(fitpoly,  which = c(1,2))
ggplot(Boston, aes(DIS, NOX)) +
geom_point() + theme_bw() +
stat_smooth(method = lm, formula = y ~ poly(x, raw = TRUE), col = "red")
library(ggplot2)
ggplot(Boston, aes(DIS, NOX)) +
geom_point() + theme_bw() +
stat_smooth(method = lm, formula = y ~ poly(x, raw = TRUE), col = "red")
ggplot(boston, aes(DIS, NOX)) +
geom_point() + theme_bw() +
stat_smooth(method = lm, formula = y ~ poly(x, raw = TRUE), col = "red")
library(ggplot2)
boston = read.csv("boston.csv")
fitpoly = glm(NOX ~ poly(DIS, 2, raw = TRUE), data = boston)
par(mfrow = c(1,2))
plot(fitpoly,  which = c(1,2))
ggplot(boston, aes(DIS, NOX)) +
geom_point() + theme_bw() +
stat_smooth(method = lm, formula = y ~ poly(x, 2, raw = TRUE), col = "red")
ggplot(boston, aes(DIS, NOX)) +
geom_point() + theme_bw() +
stat_smooth(method = lm, formula = y ~ poly(x, 2, raw = TRUE), col = "red")+
stat_smooth(method = lm, formula = y ~ poly(x, 3, raw = TRUE), col = "green")+
stat_smooth(method = lm, formula = y ~ poly(x, 4, raw = TRUE), col = "blue")+
stat_smooth(method = lm, formula = y ~ poly(x, 5, raw = TRUE), col = "yellow")+
stat_smooth(method = lm, formula = y ~ poly(x, 6, raw = TRUE), col = "purple")+
stat_smooth(method = lm, formula = y ~ poly(x, 7, raw = TRUE), col = "black")+
stat_smooth(method = lm, formula = y ~ poly(x, 8, raw = TRUE), col = "grey")+
stat_smooth(method = lm, formula = y ~ poly(x, 9, raw = TRUE), col = "pink")+
stat_smooth(method = lm, formula = y ~ poly(x, 10, raw = TRUE), col = "lightblue")
# Fit polynomial models and compute RSS
rss_data <- data.frame(Degree = 2:10, RSS = NA)
for (degree in rss_data$Degree) {
model <- lm(NOX ~ poly(DIS, degree, raw = TRUE), data = boston)
residuals <- model$residuals
rss_data$RSS[rss_data$Degree == degree] <- sum(residuals^2)
}
# Plot RSS vs. Polynomial Degree
ggplot(rss_data, aes(x = Degree, y = RSS)) +
geom_line(size = 1, color = "blue") +
geom_point(size = 2, color = "red") +
theme_bw() +
labs(
title = "Residual Sum of Squares (RSS) vs Polynomial Degree",
x = "Polynomial Degree",
y = "Residual Sum of Squares (RSS)"
)
library(caret)
library(caret)
library(dplyr)
training_obs <- boston$NOX %>% createDataPartition(p = 0.8, list = FALSE)
train <- boston[training_obs, ]
test <- boston[-training_obs, ]
for (i in 1:10){
# Build the linear regression model on the training set
model <- lm(NOX ~ poly(DIS, i, raw = TRUE), data = train)
# Use the model to make predictions on the test set
predictions <- model %>% predict(test)
#Examine R-squared, RMSE, and MAE of predictions
data.frame(R_squared = R2(predictions, test$NOX),
RMSE = RMSE(predictions, test$NOX),
MAE = MAE(predictions, test$NOX))
}
colnames(metrics) <- c('R2', 'RMSE', 'MAE')
metrics = data.frame(matrix(ncol = 3, nrow = 10))
colnames(metrics) <- c('R2', 'RMSE', 'MAE')
for (i in 1:10){
# Build the linear regression model on the training set
model <- lm(NOX ~ poly(DIS, i, raw = TRUE), data = train)
# Use the model to make predictions on the test set
predictions <- model %>% predict(test)
#Examine R-squared, RMSE, and MAE of predictions
metrics[i, "R2"] = (R2(predictions, test$NOX))
metrics[i, "RMSE"] = (RMSE(predictions, test$NOX))
metrics[i, "MAE"] = (MAE(predictions, test$NOX))
# RMSE = RMSE(predictions, test$NOX),
# MAE = MAE(predictions, test$NOX))
}
# select the best model(s)
which.min(metrics[, "R2"])
# select the best model(s)
which.max(metrics[, "R2"])
# select the best model(s)
indexr2 = which.max(metrics[, "R2"])
indexmae= which.min(metrics[, "MAE"])
indexrmse = which.min(metrics[, "RMSE"])
c(indexr2, indexrmse, indexmae)
quantile(boston$DIS)
knots = quantile(boston$dis)
knots = quantile(boston$DIS)
knots = quantile(boston$DIS)[2:4]
knots = quantile(boston$DIS)[2:4]
fit.spline <- lm(NIX ~ bs(DIS, knots = knots), data = boston)
fit.spline <- lm(NOX ~ bs(DIS, knots = knots), data = boston)
library(splines)
knots = quantile(boston$DIS)[2:4]
fit.spline <- lm(NOX ~ bs(DIS, knots = knots), data = boston)
summary(fit.spline)
?bs
library(splines)
knots = quantile(boston$DIS)[2:4]
fit.spline <- lm(NOX ~ bs(DIS, df = 4), data = boston)
summary(fit.spline)
ggplot(boston, aes(x = DIS, y = NOX)) +
geom_line(size = 1, color = "blue") +
geom_point(size = 2, color = "red") +
theme_bw() +
labs(
title = "Regression spline fit with 4 df",
x = "DIS",
y = "NOX"
)
ggplot(boston, aes(DIS, NOX)) +
geom_point() + theme_bw() +
stat_smooth(method = lm, formula = y ~ bs(x, df = 4), col = "red")+
```
ggplot(boston, aes(DIS, NOX)) +
geom_point() + theme_bw() +
stat_smooth(method = lm, formula = y ~ bs(x, df = 4), col = "red")
?bs
debugonce(fit.spline)
debugonce(lm(NOX ~ bs(DIS, df = 4), data = boston))
fit.spline <- lm(NOX ~ bs(DIS, df = 4), data = boston)
debugonce(lm)
summary(fit.spline)
library(splines)
knots = quantile(boston$DIS)[2:4]
debug()
debugonce(lm)
fit.spline <- lm(NOX ~ bs(DIS, df = 4), data = boston)
undebug()
library(splines)
knots = quantile(boston$DIS)[2:4]
debugonce(bs)
fit.spline <- lm(NOX ~ bs(DIS, df = 4), data = boston)
summary(fit.spline)
fit.spline$coefficients
fit.spline$model
clear
metrics_splines = data.frame(matrix(ncol = 3, nrow = 10))
colnames(metrics) <- c('R2', 'RMSE', 'MAE')
View(metrics)
metrics_splines = data.frame(matrix(ncol = 3, nrow = 10))
colnames(metrics_splines) <- c('R2', 'RMSE', 'MAE')
for (i in 1:10){
# Build the linear regression model on the training set
model <- lm(NOX ~ bs(DIS, df = i), data = train)
# Use the model to make predictions on the test set
predictions <- model %>% predict(test)
#Examine R-squared, RMSE, and MAE of predictions
metrics_splines[i, "R2"] = (R2(predictions, test$NOX))
metrics_splines[i, "RMSE"] = (RMSE(predictions, test$NOX))
metmetrics_splinesrics[i, "MAE"] = (MAE(predictions, test$NOX))
# RMSE = RMSE(predictions, test$NOX),
# MAE = MAE(predictions, test$NOX))
}
metrics_splines = data.frame(matrix(ncol = 3, nrow = 10))
colnames(metrics_splines) <- c('R2', 'RMSE', 'MAE')
for (i in 1:10){
# Build the linear regression model on the training set
model <- lm(NOX ~ bs(DIS, df = i), data = train)
# Use the model to make predictions on the test set
predictions <- model %>% predict(test)
#Examine R-squared, RMSE, and MAE of predictions
metrics_splines[i, "R2"] = (R2(predictions, test$NOX))
metrics_splines[i, "RMSE"] = (RMSE(predictions, test$NOX))
metrics_splines[i, "MAE"] = (MAE(predictions, test$NOX))
# RMSE = RMSE(predictions, test$NOX),
# MAE = MAE(predictions, test$NOX))
}
# select the best model(s)
indexr2 = which.max(metrics[, "R2"])
indexrmse = which.min(metrics[, "RMSE"])
indexmae= which.min(metrics[, "MAE"])
# select the best model(s)
indexr2 = which.max(metrics_splines[, "R2"])
indexrmse = which.min(metrics_splines[, "RMSE"])
indexmae= which.min(metrics_splines[, "MAE"])
c(indexr2, indexrmse, indexmae)
seq(1, 500, by = 5)
for (i in 1:seq(1, 500, by = 5)){
# Build the linear regression model on the training set
model <- lm(NOX ~ bs(DIS, df = i), data = train)
# Use the model to make predictions on the test set
predictions <- model %>% predict(test)
#Examine R-squared, RMSE, and MAE of predictions
metrics_splines[i, "R2"] = (R2(predictions, test$NOX))
metrics_splines[i, "RMSE"] = (RMSE(predictions, test$NOX))
metrics_splines[i, "MAE"] = (MAE(predictions, test$NOX))
# RMSE = RMSE(predictions, test$NOX),
# MAE = MAE(predictions, test$NOX))
}
# select the best model(s)
indexr2 = which.max(metrics_splines[, "R2"])
indexrmse = which.min(metrics_splines[, "RMSE"])
indexmae= which.min(metrics_splines[, "MAE"])
for (i in 1:seq(1, 100, by = 5)){
# Build the linear regression model on the training set
model <- lm(NOX ~ bs(DIS, df = i), data = train)
# Use the model to make predictions on the test set
predictions <- model %>% predict(test)
#Examine R-squared, RMSE, and MAE of predictions
metrics_splines[i, "R2"] = (R2(predictions, test$NOX))
metrics_splines[i, "RMSE"] = (RMSE(predictions, test$NOX))
metrics_splines[i, "MAE"] = (MAE(predictions, test$NOX))
# RMSE = RMSE(predictions, test$NOX),
# MAE = MAE(predictions, test$NOX))
}
for (i in 1:seq(1, 50)){
# Build the linear regression model on the training set
model <- lm(NOX ~ bs(DIS, df = i), data = train)
# Use the model to make predictions on the test set
predictions <- model %>% predict(test)
#Examine R-squared, RMSE, and MAE of predictions
metrics_splines[i, "R2"] = (R2(predictions, test$NOX))
metrics_splines[i, "RMSE"] = (RMSE(predictions, test$NOX))
metrics_splines[i, "MAE"] = (MAE(predictions, test$NOX))
# RMSE = RMSE(predictions, test$NOX),
# MAE = MAE(predictions, test$NOX))
}
# select the best model(s)
indexr2 = which.max(metrics_splines[, "R2"])
indexrmse = which.min(metrics_splines[, "RMSE"])
indexmae= which.min(metrics_splines[, "MAE"])
c(indexr2, indexrmse, indexmae)
for (i in 1:50){
# Build the linear regression model on the training set
model <- lm(NOX ~ bs(DIS, df = i), data = train)
# Use the model to make predictions on the test set
predictions <- model %>% predict(test)
#Examine R-squared, RMSE, and MAE of predictions
metrics_splines[i, "R2"] = (R2(predictions, test$NOX))
metrics_splines[i, "RMSE"] = (RMSE(predictions, test$NOX))
metrics_splines[i, "MAE"] = (MAE(predictions, test$NOX))
# RMSE = RMSE(predictions, test$NOX),
# MAE = MAE(predictions, test$NOX))
}
for (i in 1:10){
# Build the linear regression model on the training set
model <- lm(NOX ~ bs(DIS, df = i), data = train)
# Use the model to make predictions on the test set
predictions <- model %>% predict(test)
#Examine R-squared, RMSE, and MAE of predictions
metrics_splines[i, "R2"] = (R2(predictions, test$NOX))
metrics_splines[i, "RMSE"] = (RMSE(predictions, test$NOX))
metrics_splines[i, "MAE"] = (MAE(predictions, test$NOX))
# RMSE = RMSE(predictions, test$NOX),
# MAE = MAE(predictions, test$NOX))
}
for (i in 1:15){
# Build the linear regression model on the training set
model <- lm(NOX ~ bs(DIS, df = i), data = train)
# Use the model to make predictions on the test set
predictions <- model %>% predict(test)
#Examine R-squared, RMSE, and MAE of predictions
metrics_splines[i, "R2"] = (R2(predictions, test$NOX))
metrics_splines[i, "RMSE"] = (RMSE(predictions, test$NOX))
metrics_splines[i, "MAE"] = (MAE(predictions, test$NOX))
# RMSE = RMSE(predictions, test$NOX),
# MAE = MAE(predictions, test$NOX))
}
for (i in 1:14){
# Build the linear regression model on the training set
model <- lm(NOX ~ bs(DIS, df = i), data = train)
# Use the model to make predictions on the test set
predictions <- model %>% predict(test)
#Examine R-squared, RMSE, and MAE of predictions
metrics_splines[i, "R2"] = (R2(predictions, test$NOX))
metrics_splines[i, "RMSE"] = (RMSE(predictions, test$NOX))
metrics_splines[i, "MAE"] = (MAE(predictions, test$NOX))
# RMSE = RMSE(predictions, test$NOX),
# MAE = MAE(predictions, test$NOX))
}
# select the best model(s)
indexr2 = which.max(metrics_splines[, "R2"])
indexrmse = which.min(metrics_splines[, "RMSE"])
indexmae= which.min(metrics_splines[, "MAE"])
for (i in 1:15){
# Build the linear regression model on the training set
model <- lm(NOX ~ bs(DIS, df = i), data = train)
# Use the model to make predictions on the test set
predictions <- model %>% predict(test)
#Examine R-squared, RMSE, and MAE of predictions
metrics_splines[i, "R2"] = (R2(predictions, test$NOX))
metrics_splines[i, "RMSE"] = (RMSE(predictions, test$NOX))
metrics_splines[i, "MAE"] = (MAE(predictions, test$NOX))
# RMSE = RMSE(predictions, test$NOX),
# MAE = MAE(predictions, test$NOX))
}
# select the best model(s)
indexr2 = which.max(metrics_splines[, "R2"])
indexrmse = which.min(metrics_splines[, "RMSE"])
indexmae= which.min(metrics_splines[, "MAE"])
c(indexr2, indexrmse, indexmae)
metrics_splines = data.frame(matrix(ncol = 3, nrow = 10))
colnames(metrics_splines) <- c('R2', 'RMSE', 'MAE')
for (i in 1:15){
# Build the linear regression model on the training set
model <- lm(NOX ~ bs(DIS, df = i), data = boston)
# Use the model to make predictions on the test set
predictions <- model %>% predict(boston)
#Examine R-squared, RMSE, and MAE of predictions
metrics_splines[i, "R2"] = (R2(predictions, test$NOX))
metrics_splines[i, "RMSE"] = (RMSE(predictions, test$NOX))
metrics_splines[i, "MAE"] = (MAE(predictions, test$NOX))
# RMSE = RMSE(predictions, test$NOX),
# MAE = MAE(predictions, test$NOX))
}
for (i in 1:15){
debug()
# Build the linear regression model on the training set
model <- lm(NOX ~ bs(DIS, df = i), data = boston)
# Use the model to make predictions on the test set
predictions <- model %>% predict(boston)
#Examine R-squared, RMSE, and MAE of predictions
metrics_splines[i, "R2"] = (R2(predictions, test$NOX))
metrics_splines[i, "RMSE"] = (RMSE(predictions, test$NOX))
metrics_splines[i, "MAE"] = (MAE(predictions, test$NOX))
# RMSE = RMSE(predictions, test$NOX),
# MAE = MAE(predictions, test$NOX))
}
for (i in 1:15){
# Build the linear regression model on the training set
model <- lm(NOX ~ bs(DIS, df = i), data = boston)
# Use the model to make predictions on the test set
predictions <- model %>% predict(boston)
#Examine R-squared, RMSE, and MAE of predictions
metrics_splines[i, "R2"] = (R2(predictions, test$NOX))
metrics_splines[i, "RMSE"] = (RMSE(predictions, test$NOX))
metrics_splines[i, "MAE"] = (MAE(predictions, test$NOX))
# RMSE = RMSE(predictions, test$NOX),
# MAE = MAE(predictions, test$NOX))
}
for (i in 1:15){
# Build the linear regression model on the training set
model <- lm(NOX ~ bs(DIS, df = i), data = boston)
# Use the model to make predictions on the test set
predictions <- model %>% predict(boston)
#Examine R-squared, RMSE, and MAE of predictions
metrics_splines[i, "R2"] = (R2(predictions, boston$NOX))
metrics_splines[i, "RMSE"] = (RMSE(predictions, boston$NOX))
metrics_splines[i, "MAE"] = (MAE(predictions, boston$NOX))
# RMSE = RMSE(predictions, test$NOX),
# MAE = MAE(predictions, test$NOX))
}
# select the best model(s)
indexr2 = which.max(metrics_splines[, "R2"])
indexrmse = which.min(metrics_splines[, "RMSE"])
indexmae= which.min(metrics_splines[, "MAE"])
metrics_splines_cr = data.frame(matrix(ncol = 3, nrow = 10))
colnames(metric_splines_cr) <- c('R2', 'RMSE', 'MAE')
metrics_splines_cr = data.frame(matrix(ncol = 3, nrow = 10))
colnames(metric_splines_cr) <- c('R2', 'RMSE', 'MAE')
colnames(metrics_splines_cr) <- c('R2', 'RMSE', 'MAE')
#define the number of subsets (or "folds") to use
train_control <- trainControl(method = "cv", number = 5)
for (i in 1:15){
# Build the linear regression model on the training set
model <- lm(NOX ~ bs(DIS, df = i), data = boston, trControl = train_control)
# Use the model to make predictions on the test set
predictions <- model %>% predict(boston)
#Examine R-squared, RMSE, and MAE of predictions
metrics_splines_cr[i, "R2"] = (R2(predictions, boston$NOX))
metrics_splines_cr[i, "RMSE"] = (RMSE(predictions, boston$NOX))
metrics_splines_cr[i, "MAE"] = (MAE(predictions, boston$NOX))
# RMSE = RMSE(predictions, test$NOX),
# MAE = MAE(predictions, test$NOX))
}
# select the best model(s)
indexr2 = which.max(metrics_splines_cr[, "R2"])
indexrmse = which.min(metrics_splines_cr[, "RMSE"])
indexmae= which.min(metrics_splines_cr[, "MAE"])
c(indexr2, indexrmse, indexmae)
# Load data from the MASS package
library(MASS)
data(mcycle)
head(mcycle)
# Use gam for univariate smoothing
library(mgcv)
?gam
fit_gam <- gam(accel ~ s(times, k = 30), data = mcycle, method = "GCV.Cp")
plot(fit_gam, residuals = TRUE, pch = 1, rug = FALSE)
ggplot(mcycle, aes(accel, time)) +
geom_point() + theme_bw() +
stat_smooth(method = lm, formula = y ~ gam(s(x, k = 30), col = "red"))
ggplot(mcycle, aes(accel, times)) +
geom_point() + theme_bw() +
stat_smooth(method = lm, formula = y ~ gam(s(x, k = 30), col = "red"))
ggplot(mcycle, aes(times, accel)) +
geom_point() + theme_bw() +
stat_smooth(method = lm, formula = y ~ gam(s(x, k = 30), col = "red"))
ggplot(mcycle, aes(times, accel)) +
geom_point() + theme_bw() +
stat_smooth(method = gam, formula = y ~ s(x, k = 30), col = "red")
library(car)
install.packages("car")
```{r}
plot(fit_gam, residuals = TRUE, pch = 1, rug = FALSE)
fit_gam$df.residual
summary(fit_gam)
# Fit a polynomial model with lm
gam_df = fit_gam$edf
# Fit a polynomial model with lm
gam_df = fit_gam$df.residual
fit_poly <- lm(accel ~ poly(times, degree = gam_df), data = mcycle)
summary(fit_gam)
# Fit a polynomial model with lm
gam_df = 13.78
fit_poly <- lm(accel ~ poly(times, degree = gam_df), data = mcycle)
termplot(fit_poly, partial.resid = TRUE)
# Fit un-penalized thin plate regression spline
fit_tprs <- gam(accel ~ s(times, bs = "tp", k = 30), data = mcycle, sp = 0)
plot(fit_tprs, residuals = TRUE, pch = 1, rug = FALSE)
?termplot
termplot(fit_poly, partial.resid = FALSE)
termplot(fit_poly, partial.resid = FALSE)
termplot(fit_poly, partial.resid = TRUE)
# Fit un-penalized cubic regression spline
fit_crs <- gam(accel ~ s(times, bs = "cr", k = 30), data = mcycle, sp = 0)
plot(fit_crs, residuals = TRUE, pch = 1, rug = FALSE)
# Plot residuals
par(mfrow = c(2, 2))
plot(fit_gam$residuals ~ mcycle$times, main = "GAM Residuals")
plot(fit_poly$residuals ~ mcycle$times, main = "Polynomial Residuals")
plot(fit_tprs$residuals ~ mcycle$times, main = "TPRS Residuals")
plot(fit_crs$residuals ~ mcycle$times, main = "CRS Residuals")
# Plot residuals
par(mfrow = c(4, 4))
plot(fit_gam$residuals ~ mcycle$times, main = "GAM Residuals")
plot(fit_poly$residuals ~ mcycle$times, main = "Polynomial Residuals")
plot(fit_tprs$residuals ~ mcycle$times, main = "TPRS Residuals")
plot(fit_crs$residuals ~ mcycle$times, main = "CRS Residuals")
plot(fit_gam$residuals ~ mcycle$times, main = "GAM Residuals")
plot(fit_poly$residuals ~ mcycle$times, main = "Polynomial Residuals")
plot(fit_tprs$residuals ~ mcycle$times, main = "TPRS Residuals")
plot(fit_crs$residuals ~ mcycle$times, main = "CRS Residuals")
library(gridExtra)
library(ggplot2)
# Create individual plots using ggplot
plot1 <- ggplot(mcycle, aes(x = times, y = fit_gam$residuals)) +
geom_point() +
labs(title = "GAM Residuals", x = "Time", y = "Residuals")
plot2 <- ggplot(mcycle, aes(x = times, y = fit_poly$residuals)) +
geom_point() +
labs(title = "Polynomial Residuals", x = "Time", y = "Residuals")
plot3 <- ggplot(mcycle, aes(x = times, y = fit_tprs$residuals)) +
geom_point() +
labs(title = "TPRS Residuals", x = "Time", y = "Residuals")
plot4 <- ggplot(mcycle, aes(x = times, y = fit_crs$residuals)) +
geom_point() +
labs(title = "CRS Residuals", x = "Time", y = "Residuals")
# Arrange plots in a grid
grid.arrange(plot1, plot2, plot3, plot4, nrow = 2, ncol = 2)
# Fit B-spline model
library(splines)
fit_bs <- lm(accel ~ bs(times, knots = c(10, 20, 30)), data = mcycle)
termplot(fit_bs, partial.resid = TRUE)
plot4 <- ggplot(mcycle, aes(x = times, y = fit_bs$residuals)) +
geom_point() +
labs(title = "CRS Residuals", x = "Time", y = "Residuals")
plot4
plot5 <- ggplot(mcycle, aes(x = times, y = fit_bs$residuals)) +
geom_point() +
labs(title = "CRS Residuals", x = "Time", y = "Residuals")
plot5
plot5 <- ggplot(mcycle, aes(x = times, y = fit_bs$residuals)) +
geom_point() +
labs(title = "BS Residuals", x = "Time", y = "Residuals")
plot5
