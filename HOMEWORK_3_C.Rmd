---
title: "Homework 3 Group C"
author: "Billo, "
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# FSDS: Chapter 4

## Exercise 4.24
Refer to the vegetarian survey result in Exercise 4.6, with n = 25 and no vegetarians.
(a) Find the Bayesian estimate of π using a beta prior distribution with α = β equal (i) 0.5,
(ii) 1.0, (iii) 10.0. Explain how the choice of prior distribution affects the posterior mean
estimate.
(b) If you were planning how to take a larger survey from the same population, explain how
you can use the posterior results of the previous survey with n = 25 based on the prior
with α = β = 1 to form the prior distribution to use with the new survey results.

### a
```{r}
# Insert your R code for Exercise 4.24 here
bayesian_estimate = function(alpha, beta, n, x){
  v = c(alpha + x, beta + n -x)
  return(v)
}
alpha_beta = c(0.5, 1, 10)

results = c()
n = 25
x = 0
for (i in 1:length(alpha_beta)){
  results = c(results, bayesian_estimate(alpha_beta[i], alpha_beta[i], n, x))
}

results
```
The posterior mean estimate is strongly affected by the choice of the prior distribution and its parameters: here we see that the mean estimate of the distribution $\pi$ changes according to its parameters, as there is no x from the posterior to influence it in this case. In this setting where the target group is non-sampled (there are 0 vegetarians), the variance of the distribution is very inflated by the $n$ parameter.

```{r}
y <- c(1, 4, 6, 12, 13, 14, 18, 19, 20, 22, 23, 24, 26, 31, 34,
37, 46, 47, 56, 61, 63, 65, 70, 97, 385)
n <- length(y); set.seed(1989);
B <- 10^4
boot.sample <- matrix(NA, nrow = B, ncol = n)
boot.sample[1,] <- sample(y, n, replace = TRUE)
boot.sample[2,] <- sample(y, n, replace = TRUE)
boot.sample[1, ] # sample output
# bootstrap mean estimates
for(i in 1:B) {
    boot.sample[i,] <- sample(y, n, replace = TRUE)
}
boot.stat <- rowMeans(boot.sample)
hist(boot.stat, main="", breaks=20, prob=TRUE, col=gray(0.7),
xlim=c(0, 200), ylim=c(0, 0.04))

```

###b
We can use the posterior results from the previous survey as a prior: this will then be
```{r}
alpha = beta = 1
posterior_mean = alpha/(alpha + beta + n)
posterior_variance = ((alpha + x)*(beta + n -x))/((alpha + beta + n)^2 * (alpha + beta + n + 1))
c(posterior_mean, posterior_variance)
```
So a prior distribution of $\pi(0.037037037, 0.001273761)$

## Exercise 4.62
For the bootstrap method, explain the similarity and difference between the true sampling
distribution of $ \hat \theta $ and the empirically-generated bootstrap distribution in terms of its centerand its spread.

The two sampling distributions become increasingly similar as $n \to \infty$, meaning that $ (\hat \theta - \theta) \to 0$. However its values vary around $\hat \theta$ instead of the true distribution parameter(s) $\theta$.
```{r}
y <- c(1, 4, 6, 12, 13, 14, 18, 19, 20, 22, 23, 24, 26, 31, 34,
37, 46, 47, 56, 61, 63, 65, 70, 97, 385)
n <- length(y); set.seed(1989);
B <- 10^4
boot.sample <- matrix(NA, nrow = B, ncol = n)
boot.sample[1,] <- sample(y, n, replace = TRUE)
boot.sample[2,] <- sample(y, n, replace = TRUE)
boot.sample[1, ] # sample output

# bootstrap mean estimates
for(i in 1:B) {
    boot.sample[i,] <- sample(y, n, replace = TRUE)
}
boot.stat <- rowMeans(boot.sample)
hist(boot.stat, main="", breaks=20, prob=TRUE, col=gray(0.7),
xlim=c(0, 200), ylim=c(0, 0.04))

# actual mean estimates
B <- 10^4;
simu.sample <- matrix(NA, nrow = B, ncol = n)
for(i in 1:B) simu.sample[i,] <- mean(30 * exp(rnorm(n)))
simu.stat <- rowMeans(simu.sample)
hist(simu.stat, main="", breaks=20, prob=TRUE, col=gray(0.7),
xlim=c(0, 200), ylim=c(0, 0.04))
c(mean(boot.sample), mean(simu.sample))
```
The two parameters are close but there still is a significant difference.

# FSDS: Chapter 8

## Exercise 8.4
Refer to Exercise 8.1. Construct a classification tree, and prune strongly until the tree uses
a single explanatory variable. Which crabs were predicted to have satellites? How does the
proportion of correct predictions compare with the more complex tree in Figure 8.2?

```{r}
library(rpart)
library(rpart.plot)
crabs = read.table("https://stat4ds.rwth-aachen.de/data/Crabs.dat", header = TRUE)

crabs.rpart <- rpart( y~weight + color, data = crabs, method = "class")
crabs.rpart

printcp(crabs.rpart)

prune(crabs.rpart, cp =0)
printcp(crabs.rpart)
rpart.plot(crabs.rpart)

```
When pruned to just 1 variable, the model retains __weight__. Crabs that have a weight greater than 2.7 for sure have satellites
```{r}
sample <- sample(c(TRUE, FALSE), nrow(crabs), replace=TRUE, prob=c(0.8,0.2))
train  <- crabs[sample, ]
test   <- crabs[!sample, ]

tree1 = rpart( y~weight, data = train, method = "class", cp = 0)
tree2 = rpart( y~weight + color + width, data = train, method = "class")

pred1 = predict(tree1, newdata = test, type = "class")
pred2 = predict(tree2, newdata = test, type = "class")

t1 = table(pred1, test$y)
t2 = table(pred2, test$y)

prec = function(table){
  res = table[1]/(table[1] + table[2])
}
recall = function(table){
  res = table[1]/(table[1] + table[3])
}


c(prec(t1), recall(t1), prec(t2), recall(t2))
```
Our results are mixed: the simpler model registers better precision but worse recall than the second one.

# LAB: Modeling Phone Call Lengths

## Task 1: Problem description
Suppose you receive \(n = 15\) phone calls in a day, and you want to build a model to assess their average length. Your likelihood for each call length is \(y_i \sim \text{Exponential}(\lambda)\). Select an appropriate prior \(\pi(\lambda)\) and compute the posterior.

1. \(\pi(\lambda) = \text{Beta}(4, 2)\)
2. \(\pi(\lambda) = \text{Normal}(1, 2)\)
3. \(\pi(\lambda) = \text{Gamma}(4, 2)\)

For such a situation, where the likelihood is a poisson, a Gamma prior is appropriate. Due to conjugacy, this entails that the posterior will also be Gamma.
The prior is $\Gamma$(4, 2)

$$
\pi(\lambda) = \frac{\beta^{\alpha}}{\Gamma(\alpha)}\lambda^{\alpha -1}e^{- \beta \lambda }
$$
Which with our numbers becomes
$$
\pi(\lambda) = \frac{2^{4}}{\Gamma(4)}2^{4 -1}e^{-2 \lambda}
$$
During 
Our likelihood is a Poisson with $n$ = 15

$$
L = \lambda^n e^{-\lambda \sum_{i = 1}^n y_i} 
$$
Which with our numbers becomes
$$
log(L) =  2^{15} e^{-2 \sum_{i = 1}^n y_i}
$$
Our posterioir is defined as
$$
\pi (\lambda | y) = \pi(\lambda)L(\lambda|y)
$$
which (ignoring the constant $\frac{2^{4}}{\Gamma(4)}$ with our quantities becomes 
$$
\pi (\lambda | y) = \lambda^{\alpha + n - 1} e^{\lambda (\beta + \sum y_i)}
$$
We can observe that our posterior is still a Gamma distribution with $\alpha' = \alpha + n$ and $\beta' = \beta + \sum y_i$
Substituting these quantities with our numbers, we get:
$$
\pi (\lambda | y) = \lambda^{4 + 15 - 1} e^{\lambda (2 + \sum y_i)}
$$
We can now compute the actual quantities (!!! idk if this makes sense)
```{r}
n = 15:
alpha_prior = 4
beta_prior = 2
y = rpois(n, beta)
alpha_posterior = alpha + n
beta_posterior = 2 + sum(y)

prior = ((beta^alpha)/dgamma(beta, shape = beta)) * beta^(alpha - 1) * exp(-beta*sum(y))
L = 2^15 * exp(-2*sum(y))

posterior = prior*L
```

### Results
*Discuss the posterior distribution.*

# ISLR: Chapter 6

## Exercise 6.9 (a)-(d)

### Task 1: Problem description
*Provide a short description of the problem.*

### Code
```{r}
# Insert your R code for Exercise 6.9 here
```

### Results
*Discuss the results or observations from the code.*

# ISLR: Chapter 7

## Exercise 7.9
This question uses the variables dis (the weighted mean of distances
to five Boston employment centers) and nox (nitrogen oxides concen-
tration in parts per 10 million) from the Boston data. We will treat
dis as the predictor and nox as the response.
(a) Use the poly() function to fit a cubic polynomial regression to predict nox using dis. Report the regression output, and plot the resulting data and polynomial fits.
```{r}
library(ggplot2)
boston = read.csv("boston.csv")
fitpoly = glm(NOX ~ poly(DIS, 2, raw = TRUE), data = boston)
par(mfrow = c(1,2))
plot(fitpoly,  which = c(1,2))

ggplot(boston, aes(DIS, NOX)) + 
  geom_point() + theme_bw() + 
  stat_smooth(method = lm, formula = y ~ poly(x, 2, raw = TRUE), col = "red")
```

(b) Plot the polynomial fits for a range of different polynomial degrees (say, from 1 to 10), and report the associated residual sum of squares.
```{r}
ggplot(boston, aes(DIS, NOX)) + 
  geom_point() + theme_bw() + 
  stat_smooth(method = lm, formula = y ~ poly(x, 2, raw = TRUE), col = "red")+ 
  stat_smooth(method = lm, formula = y ~ poly(x, 3, raw = TRUE), col = "green")+ 
  stat_smooth(method = lm, formula = y ~ poly(x, 4, raw = TRUE), col = "blue")+ 
  stat_smooth(method = lm, formula = y ~ poly(x, 5, raw = TRUE), col = "yellow")+ 
  stat_smooth(method = lm, formula = y ~ poly(x, 6, raw = TRUE), col = "purple")+ 
  stat_smooth(method = lm, formula = y ~ poly(x, 7, raw = TRUE), col = "black")+ 
  stat_smooth(method = lm, formula = y ~ poly(x, 8, raw = TRUE), col = "grey")+ 
  stat_smooth(method = lm, formula = y ~ poly(x, 9, raw = TRUE), col = "pink")+ 
  stat_smooth(method = lm, formula = y ~ poly(x, 10, raw = TRUE), col = "lightblue")

# Fit polynomial models and compute RSS
rss_data <- data.frame(Degree = 2:10, RSS = NA)

for (degree in rss_data$Degree) {
  model <- lm(NOX ~ poly(DIS, degree, raw = TRUE), data = boston)
  residuals <- model$residuals
  rss_data$RSS[rss_data$Degree == degree] <- sum(residuals^2)
}

# Plot RSS vs. Polynomial Degree
ggplot(rss_data, aes(x = Degree, y = RSS)) +
  geom_line(size = 1, color = "blue") +
  geom_point(size = 2, color = "red") +
  theme_bw() +
  labs(
    title = "Residual Sum of Squares (RSS) vs Polynomial Degree",
    x = "Polynomial Degree",
    y = "Residual Sum of Squares (RSS)"
  )
```

Obviously, in such a setting, the more complicated model has the lowest error, but probably overfits the data. 
Let's test it in a useful way with Cross Validation. 

(c) Perform cross-validation or another approach to select the optimal degree for the polynomial, and explain your results.
```{r}
library(caret)
library(dplyr)

training_obs <- boston$NOX %>% createDataPartition(p = 0.8, list = FALSE)

train <- boston[training_obs, ]
test <- boston[-training_obs, ]

metrics = data.frame(matrix(ncol = 3, nrow = 10))
colnames(metrics) <- c('R2', 'RMSE', 'MAE')

for (i in 1:10){
  # Build the linear regression model on the training set
  model <- lm(NOX ~ poly(DIS, i, raw = TRUE), data = train)

  # Use the model to make predictions on the test set
  predictions <- model %>% predict(test)

  #Examine R-squared, RMSE, and MAE of predictions
  metrics[i, "R2"] = (R2(predictions, test$NOX))
  metrics[i, "RMSE"] = (RMSE(predictions, test$NOX))
  metrics[i, "MAE"] = (MAE(predictions, test$NOX))


           # RMSE = RMSE(predictions, test$NOX),
           # MAE = MAE(predictions, test$NOX))
}
# select the best model(s)
indexr2 = which.max(metrics[, "R2"])
indexrmse = which.min(metrics[, "RMSE"])
indexmae= which.min(metrics[, "MAE"])

c(indexr2, indexrmse, indexmae)
```

The cubic polynomial fit seems to be the best one in this case.

(d) Use the bs() function to fit a regression spline to predict nox using dis. Report the output for the fit using four degrees of freedom. How did you choose the knots? Plot the resulting fit.
```{r}
library(splines)
knots = quantile(boston$DIS)[2:4]
debugonce(bs)
fit.spline <- lm(NOX ~ bs(DIS, df = 4), data = boston)
summary(fit.spline)
```
The knots were chosen by the function itself:
```{r}
knots <- seq.int(from = 0, to = 1, length.out = nIknots + 
        2L)[-c(1L, nIknots + 2L)]
      quantile(x[!outside], knots)
```

```{r}

ggplot(boston, aes(DIS, NOX)) + 
  geom_point() + theme_bw() + 
  stat_smooth(method = lm, formula = y ~ bs(x, df = 4), col = "red")

```

(e) Now fit a regression spline for a range of degrees of freedom, and plot the resulting fits and report the resulting RSS. Describe the results obtained.
```{r}
metrics_splines = data.frame(matrix(ncol = 3, nrow = 10))
colnames(metrics_splines) <- c('R2', 'RMSE', 'MAE')

for (i in 1:15){
  # Build the linear regression model on the training set
  model <- lm(NOX ~ bs(DIS, df = i), data = boston)

  # Use the model to make predictions on the test set
  predictions <- model %>% predict(boston)

  #Examine R-squared, RMSE, and MAE of predictions
  metrics_splines[i, "R2"] = (R2(predictions, boston$NOX))
  metrics_splines[i, "RMSE"] = (RMSE(predictions, boston$NOX))
  metrics_splines[i, "MAE"] = (MAE(predictions, boston$NOX))


           # RMSE = RMSE(predictions, test$NOX),
           # MAE = MAE(predictions, test$NOX))
}

# select the best model(s)
indexr2 = which.max(metrics_splines[, "R2"])
indexrmse = which.min(metrics_splines[, "RMSE"])
indexmae= which.min(metrics_splines[, "MAE"])

c(indexr2, indexrmse, indexmae)
```
The best model is between with 14 degrees of freedom

(f) Perform cross-validation or another approach in order to select the best degrees of freedom for a regression spline on this data. Describe your results.
```{r}
metrics_splines_cr = data.frame(matrix(ncol = 3, nrow = 10))
colnames(metrics_splines_cr) <- c('R2', 'RMSE', 'MAE')

#define the number of subsets (or "folds") to use
train_control <- trainControl(method = "cv", number = 5)

for (i in 1:15){
  # Build the linear regression model on the training set
  model <- lm(NOX ~ bs(DIS, df = i), data = boston, trControl = train_control)

  # Use the model to make predictions on the test set
  predictions <- model %>% predict(boston)

  #Examine R-squared, RMSE, and MAE of predictions
  metrics_splines_cr[i, "R2"] = (R2(predictions, boston$NOX))
  metrics_splines_cr[i, "RMSE"] = (RMSE(predictions, boston$NOX))
  metrics_splines_cr[i, "MAE"] = (MAE(predictions, boston$NOX))


           # RMSE = RMSE(predictions, test$NOX),
           # MAE = MAE(predictions, test$NOX))
}

# select the best model(s)
indexr2 = which.max(metrics_splines_cr[, "R2"])
indexrmse = which.min(metrics_splines_cr[, "RMSE"])
indexmae= which.min(metrics_splines_cr[, "MAE"])

c(indexr2, indexrmse, indexmae)
```

The result is the same as without cross-validation

# GAM: Univariate Smoothing with mcycle Data

### Task 1: Load and Inspect Data
```{r}
# Load data from the MASS package
library(MASS)
data(mcycle)
head(mcycle)
```

### Task 2: Plot Data and Fit GAM Model
```{r}
# Use gam for univariate smoothing
library(mgcv)
fit_gam <- gam(accel ~ s(times, k = 30), data = mcycle, method = "GCV.Cp")
ggplot(mcycle, aes(times, accel)) + 
  geom_point() + theme_bw() + 
  stat_smooth(method = gam, formula = y ~ s(x, k = 30), col = "red")
```

```{r}
plot(fit_gam, residuals = TRUE, pch = 1, rug = FALSE)

```

### Task 3: Fit Polynomial Model
```{r}
# Fit a polynomial model with lm
gam_df = 13.78
fit_poly <- lm(accel ~ poly(times, degree = gam_df), data = mcycle)
termplot(fit_poly, partial.resid = TRUE)
```

### Task 4: Compare with Un-penalized Thin Plate Regression Spline
```{r}
# Fit un-penalized thin plate regression spline
fit_tprs <- gam(accel ~ s(times, bs = "tp", k = 30), data = mcycle, sp = 0)
plot(fit_tprs, residuals = TRUE, pch = 1, rug = FALSE)
```

### Task 5: Compare with Un-penalized Cubic Regression Spline
```{r}
# Fit un-penalized cubic regression spline
fit_crs <- gam(accel ~ s(times, bs = "cr", k = 30), data = mcycle, sp = 0)
plot(fit_crs, residuals = TRUE, pch = 1, rug = FALSE)
```

### Task 6: Model Residuals
```{r}
# Plot residuals
par(mfrow = c(4, 4))
plot(fit_gam$residuals ~ mcycle$times, main = "GAM Residuals")
plot(fit_poly$residuals ~ mcycle$times, main = "Polynomial Residuals")
plot(fit_tprs$residuals ~ mcycle$times, main = "TPRS Residuals")
plot(fit_crs$residuals ~ mcycle$times, main = "CRS Residuals")
```
```{r}
library(gridExtra)
library(ggplot2)

# Create individual plots using ggplot
plot1 <- ggplot(mcycle, aes(x = times, y = fit_gam$residuals)) +
  geom_point() +
  labs(title = "GAM Residuals", x = "Time", y = "Residuals")

plot2 <- ggplot(mcycle, aes(x = times, y = fit_poly$residuals)) +
  geom_point() +
  labs(title = "Polynomial Residuals", x = "Time", y = "Residuals")

plot3 <- ggplot(mcycle, aes(x = times, y = fit_tprs$residuals)) +
  geom_point() +
  labs(title = "TPRS Residuals", x = "Time", y = "Residuals")

plot4 <- ggplot(mcycle, aes(x = times, y = fit_crs$residuals)) +
  geom_point() +
  labs(title = "CRS Residuals", x = "Time", y = "Residuals")

# Arrange plots in a grid
grid.arrange(plot1, plot2, plot3, plot4, nrow = 2, ncol = 2)

```
The residuals with TPRS and CRS are a little bit more sparse but there don't seem to be substantial improvements of one model over another.

### Task 7: Fit B-spline Model
```{r}
# Fit B-spline model
library(splines)
fit_bs <- lm(accel ~ bs(times, knots = c(10, 20, 30)), data = mcycle)
termplot(fit_bs, partial.resid = TRUE)
```
```{r}
plot5 <- ggplot(mcycle, aes(x = times, y = fit_bs$residuals)) +
  geom_point() +
  labs(title = "BS Residuals", x = "Time", y = "Residuals")
plot5

```
B-splines seem to leave some pattern in the data. The other models were an overall better fit. 
